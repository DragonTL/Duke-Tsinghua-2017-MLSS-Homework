{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Assignment: Multilayer Perceptron (MLP) and Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: [Long Tian]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've run through a simple logistic regression model on MNIST, let's see if we can do better (Hint: we can). For this assignment, you'll build a multilayer perceptron (MLP) and a convolutional neural network (CNN), two popular types of neural networks, and compare their performance. Some potentially useful code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron\n",
    "\n",
    "Build a multilayer perceptron for MNIST digit classfication. Feel free to play around with the model architecture and see how the training time/performance changes, but to begin, try the following:\n",
    "\n",
    "Image -> fully connected (500 hidden units) -> nonlinearity (Sigmoid/ReLU) -> fully connected (10 hidden units) -> softmax\n",
    "\n",
    "Skeleton framework for you to fill in (Code you need to provide is marked by `###`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ReLu activation function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, validation accuracy 0.097\n",
      "step 250, validation accuracy 0.909\n",
      "step 500, validation accuracy 0.912\n",
      "step 750, validation accuracy 0.942\n",
      "step 1000, validation accuracy 0.96\n",
      "step 1250, validation accuracy 0.963\n",
      "step 1500, validation accuracy 0.964\n",
      "step 1750, validation accuracy 0.961\n",
      "step 2000, validation accuracy 0.955\n",
      "step 2250, validation accuracy 0.97\n",
      "step 2500, validation accuracy 0.966\n",
      "step 2750, validation accuracy 0.962\n",
      "step 3000, validation accuracy 0.965\n",
      "step 3250, validation accuracy 0.948\n",
      "step 3500, validation accuracy 0.975\n",
      "step 3750, validation accuracy 0.975\n",
      "test accuracy 0.9673\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Model Inputs\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "n_sample = mnist.train.images.shape[0]\n",
    "n_input = mnist.train.images.shape[1]\n",
    "n_hidden = 500\n",
    "n_class = mnist.train.labels.shape[1]\n",
    "\n",
    "x = tf.placeholder('float', [None, mnist.train.images.shape[1]]) ### MNIST images enter graph here ###\n",
    "y_ = tf.placeholder('float', [None, mnist.train.labels.shape[1]]) ### MNIST labels enter graph here ###\n",
    "\n",
    "learning_rate = 1\n",
    "training_epochs = 4000\n",
    "batch_size = 50\n",
    "\n",
    "weight = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden], stddev = 0.1)),\n",
    "    'h2': tf.Variable(tf.truncated_normal([n_hidden, n_class], stddev = 0.1))\n",
    "}\n",
    "bias = {\n",
    "    'h1': tf.Variable(tf.constant(0.1, shape = [n_hidden, ])),\n",
    "    'h2': tf.Variable(tf.constant(0.1, shape = [n_class, ]))\n",
    "}\n",
    "# Define the graph\n",
    "def multiplayer_perceptron(x, weight, bias):\n",
    "\n",
    "    layerin = tf.add(tf.matmul(x, weight['h1']), bias['h1'])\n",
    "    layerout = tf.nn.relu(layerin)\n",
    "    \n",
    "    layerin = tf.add(tf.matmul(layerout, weight['h2']), bias['h2'])\n",
    "    y_mlp = tf.nn.softmax(layerin) # can only use softmax here!!!???\n",
    "\n",
    "    return y_mlp\n",
    "\n",
    "\n",
    "### Create your MLP here##\n",
    "### Make sure to name your MLP output as y_mlp ###\n",
    "y_mlp = multiplayer_perceptron(x, weight, bias)\n",
    "\n",
    "# Loss \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_mlp))\n",
    "\n",
    "# Optimizer\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y_mlp, 1), tf.argmax(y_, 1)) #evaluation on train data!\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training regimen\n",
    "    for i in range(training_epochs):\n",
    "        # Validate every 250th batch\n",
    "        if i % 250 == 0:\n",
    "            validation_accuracy = 0\n",
    "            for v in range(10):\n",
    "                batch = mnist.validation.next_batch(100)\n",
    "                validation_accuracy += (1/10) * accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            print('step %d, validation accuracy %g' % (i, validation_accuracy))\n",
    "        \n",
    "        # Train    \n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sigmoid activation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, validation accuracy 0.109\n",
      "step 250, validation accuracy 0.682\n",
      "step 500, validation accuracy 0.746\n",
      "step 750, validation accuracy 0.757\n",
      "step 1000, validation accuracy 0.794\n",
      "step 1250, validation accuracy 0.852\n",
      "step 1500, validation accuracy 0.846\n",
      "step 1750, validation accuracy 0.816\n",
      "step 2000, validation accuracy 0.823\n",
      "step 2250, validation accuracy 0.834\n",
      "step 2500, validation accuracy 0.841\n",
      "step 2750, validation accuracy 0.845\n",
      "step 3000, validation accuracy 0.853\n",
      "step 3250, validation accuracy 0.849\n",
      "step 3500, validation accuracy 0.847\n",
      "step 3750, validation accuracy 0.846\n",
      "test accuracy 0.8484\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Model Inputs\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "n_sample = mnist.train.images.shape[0]\n",
    "n_input = mnist.train.images.shape[1]\n",
    "n_hidden = 500\n",
    "n_class = mnist.train.labels.shape[1]\n",
    "\n",
    "x = tf.placeholder('float', [None, mnist.train.images.shape[1]]) ### MNIST images enter graph here ###\n",
    "y_ = tf.placeholder('float', [None, mnist.train.labels.shape[1]]) ### MNIST labels enter graph here ###\n",
    "\n",
    "learning_rate = 1\n",
    "training_epochs = 4000\n",
    "batch_size = 50\n",
    "\n",
    "weight = {\n",
    "    'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden], stddev = 0.1)),\n",
    "    'h2': tf.Variable(tf.truncated_normal([n_hidden, n_class], stddev = 0.1))\n",
    "}\n",
    "bias = {\n",
    "    'h1': tf.Variable(tf.constant(0.1, shape = [n_hidden, ])),\n",
    "    'h2': tf.Variable(tf.constant(0.1, shape = [n_class, ]))\n",
    "}\n",
    "# Define the graph\n",
    "def multiplayer_perceptron(x, weight, bias):\n",
    "\n",
    "    layerin = tf.add(tf.matmul(x, weight['h1']), bias['h1'])\n",
    "    layerout = tf.nn.sigmoid(layerin)\n",
    "    \n",
    "    layerin = tf.add(tf.matmul(layerout, weight['h2']), bias['h2'])\n",
    "    y_mlp = tf.nn.softmax(layerin) # can only use softmax here!!!???\n",
    "\n",
    "    return y_mlp\n",
    "\n",
    "\n",
    "### Create your MLP here##\n",
    "### Make sure to name your MLP output as y_mlp ###\n",
    "y_mlp = multiplayer_perceptron(x, weight, bias)\n",
    "\n",
    "# Loss \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_mlp))\n",
    "\n",
    "# Optimizer\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y_mlp, 1), tf.argmax(y_, 1)) #evaluation on train data!\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training regimen\n",
    "    for i in range(training_epochs):\n",
    "        # Validate every 250th batch\n",
    "        if i % 250 == 0:\n",
    "            validation_accuracy = 0\n",
    "            for v in range(10):\n",
    "                batch = mnist.validation.next_batch(100)\n",
    "                validation_accuracy += (1/10) * accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            print('step %d, validation accuracy %g' % (i, validation_accuracy))\n",
    "        \n",
    "        # Train    \n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "How do the sigmoid and rectified linear unit (ReLU) compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Sigmoid activation function converges more slowly than ReLu comparing each convergence procedure.Meanwhile,recognition accuracy has a big gap.That's to say,ReLu can achieve almost 97% while Sigmoid only get 85% or so.\n",
    "It may caused by different properties of two activation functions.ReLu can give a much more sparse feature space than sigmoid,and such kind of features may be effective enough for MNIST to recognize.When inputs are far away from 0,the gradient of ReLu always be bigger than sigmoid,this can help ReLu converge faster than sigmoid. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "Build a simple 2-layer CNN for MNIST digit classfication. Feel free to play around with the model architecture and see how the training time/performance changes, but to begin, try the following:\n",
    "\n",
    "Image -> CNN (32 5x5 filters) -> nonlinearity (ReLU) ->  (2x2 max pool) -> CNN (64 5x5 filters) -> nonlinearity (ReLU) -> (2x2 max pool) -> fully connected (1024 hidden units) -> nonlinearity (ReLU) -> fully connected (10 hidden units) -> softmax\n",
    "\n",
    "Some additional functions that you might find helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, validation accuracy 0.106\n",
      "step 250, validation accuracy 0.732\n",
      "step 500, validation accuracy 0.808\n",
      "step 750, validation accuracy 0.766\n",
      "step 1000, validation accuracy 0.81\n",
      "step 1250, validation accuracy 0.784\n",
      "step 1500, validation accuracy 0.874\n",
      "step 1750, validation accuracy 0.848\n",
      "step 2000, validation accuracy 0.862\n",
      "step 2250, validation accuracy 0.9\n",
      "step 2500, validation accuracy 0.886\n",
      "step 2750, validation accuracy 0.872\n",
      "step 3000, validation accuracy 0.898\n",
      "step 3250, validation accuracy 0.882\n",
      "step 3500, validation accuracy 0.862\n",
      "step 3750, validation accuracy 0.902\n",
      "step 4000, validation accuracy 0.9\n",
      "step 4250, validation accuracy 0.892\n",
      "step 4500, validation accuracy 0.888\n",
      "step 4750, validation accuracy 0.892\n",
      "step 5000, validation accuracy 0.896\n",
      "step 5250, validation accuracy 0.868\n",
      "step 5500, validation accuracy 0.87\n",
      "step 5750, validation accuracy 0.966\n",
      "step 6000, validation accuracy 0.984\n",
      "step 6250, validation accuracy 0.974\n",
      "step 6500, validation accuracy 0.982\n",
      "step 6750, validation accuracy 0.986\n",
      "step 7000, validation accuracy 0.99\n",
      "step 7250, validation accuracy 0.994\n",
      "step 7500, validation accuracy 0.988\n",
      "step 7750, validation accuracy 0.98\n",
      "step 8000, validation accuracy 0.99\n",
      "step 8250, validation accuracy 0.988\n",
      "step 8500, validation accuracy 0.99\n",
      "step 8750, validation accuracy 0.99\n",
      "step 9000, validation accuracy 0.984\n",
      "step 9250, validation accuracy 0.986\n",
      "step 9500, validation accuracy 0.994\n",
      "step 9750, validation accuracy 0.988\n",
      "test accuracy 0.9892\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Model Inputs\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Model Inputs\n",
    "x = tf.placeholder('float', [None, mnist.train.images.shape[1]])### MNIST images enter graph here ###\n",
    "y_ = tf.placeholder('float', [None, mnist.train.labels.shape[1]])### MNIST labels enter graph here ###\n",
    "\n",
    "# Helper functions for creating weight variables\n",
    "def weight_variable(shape):\n",
    "    \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "                             \n",
    "# Convolutional neural network functions\n",
    "def conv2d(x, W):\n",
    "    \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "  \n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])    \n",
    "                             \n",
    "# Define the graph\n",
    "def CNN(x_image):\n",
    "    W = tf.Variable(tf.zeros([784,10]))  \n",
    "    b = tf.Variable(tf.zeros([10]))  \n",
    "\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])  \n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "    h_pool1 = max_pool_2x2(h_conv1)  \n",
    "\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])  \n",
    "    b_conv2 = bias_variable([64])  \n",
    "\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # Now image size is reduced to 7*7  \n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])  \n",
    "    b_fc1 = bias_variable([1024])  \n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])  \n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  \n",
    "\n",
    "    W_fc2 = weight_variable([1024, 10])  \n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    y_conv = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "    return y_conv\n",
    "\n",
    "### Create your CNN here##\n",
    "### Make sure to name your CNN output as y_conv ###\n",
    "y_conv = CNN(x_image)\n",
    "\n",
    "# Loss \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "# Optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training regimen\n",
    "    for i in range(10000):\n",
    "        # Validate every 250th batch\n",
    "        if i % 250 == 0:\n",
    "            validation_accuracy = 0\n",
    "            for v in range(10):\n",
    "                batch = mnist.validation.next_batch(50)\n",
    "                validation_accuracy += (1/10) * accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            print('step %d, validation accuracy %g' % (i, validation_accuracy))\n",
    "        \n",
    "        # Train    \n",
    "        batch = mnist.train.next_batch(50)\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some differences from the logistic regression model to note:\n",
    "\n",
    "- The CNN model might take a while to train. Depending on your machine, you might expect this to take up to half an hour. If you see your validation performance start to plateau, you can kill the training.\n",
    "\n",
    "- The logistic regression model we used previously was pretty basic, and as such, we were able to get away with using the GradientDescentOptimizer, which performs implements the gradient descent algorithm. For more difficult optimization spaces (such as the ones deep networks pose), we might want to use more sophisticated algorithms. Prof David Carlson has a lecture on this later.\n",
    "    \n",
    "- Because of the larger size of our network, notice that our minibatch size has shrunk.\n",
    "    \n",
    "- We've added a validation step every 250 minibatches. This let's us see how our model is doing during the training process, rather than sit around twiddling our thumbs and hoping for the best when training finishes. This becomes especially significant as training regimens start approaching days and weeks in length. Normally, we validate on the entire validation set, but for the sake of time we'll just stick to 10 validation minibatches (500 images) for this homework assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "How do the MLP and CNN compare in accuracy? Training time? Why would you use one vs the other? Is there a problem you see with MLPs when applied to other image datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Train time:MLP consumes less than 1 min;CNN takes as long as half of the class time,around 30 to 35 mins.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Accuracy:MLP with ReLu is 97.7%;CNN is 98.9%，more than 1% of MLP.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "I also tried MLP with CIFAR10 dataset,which performs aweful!While CNN does well in that dataset as other people's blogs say.The reason caused this probabily is inherent differences between MLP and CNN.CNN is much more proper to handle 2D data such as image.\n",
    "\n",
    "\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
